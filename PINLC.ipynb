{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-08T15:08:21.581245Z",
     "start_time": "2024-03-08T15:08:21.568224Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pkgutil\n",
    "# if not pkgutil.find_loader(\"dreal\"):\n",
    "#   !curl https://raw.githubusercontent.com/dreal/dreal4/master/setup/ubuntu/22.04/install.sh | bash\n",
    "#   !pip install dreal --upgrade\n",
    "# if not pkgutil.find_loader(\"dreal\"):\n",
    "#   !pip install control"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utility functions for pytorch and dreal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62d139985893758e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import dreal as d\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# pytorch utils\n",
    "def torch_to_np(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "def np_to_torch(x):\n",
    "\treturn torch.from_numpy(x).float().to(device)\n",
    "\n",
    "def pipe(x, *funcs):\n",
    "\tfor f in funcs:\n",
    "\t\tx = f(x)\n",
    "\treturn x\n",
    "\n",
    "# dreal utils\n",
    "\n",
    "\n",
    "def dreal_var(n, name='x'):\n",
    "    return np.array([d.Variable(\"%s%d\" % (name, i)) for i in range(n)])\n",
    "\n",
    "def dreal_elementwise(x, func):\n",
    "    \"\"\"\n",
    "    :param x: np.array[dreal.Variable]\n",
    "    :param func: dreal function, for example, dreal.tanh\n",
    "    \"\"\"\n",
    "    return np.array([\n",
    "        func(x[i]) for i in range(len(x))\n",
    "    ])\n",
    "\n",
    "def dreal_sigmoid(x):\n",
    "    return 1 / (1 + d.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T15:08:24.133927Z",
     "start_time": "2024-03-08T15:08:23.021302Z"
    }
   },
   "id": "b58d2ac3c005a332"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dynamical System Benchmarks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce1d149435f8eac4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def rk4_step(f, x, u, dt):\n",
    "\t\"\"\" RK4 simulation for continuous-time system \"\"\"\n",
    "\tf1 = f(x, u)\n",
    "\tf2 = f(x + 0.5 * dt * f1, u)\n",
    "\tf3 = f(x + 0.5 * dt * f2, u)\n",
    "\tf4 = f(x + dt * f3, u)\n",
    "\treturn x + (dt / 6.0) * (f1 + 2 * f2 + 2 * f3 + f4)\n",
    "    \n",
    "class Benchmark():\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.name = None\n",
    "        self.nx = None\n",
    "        self.nu = None\n",
    "        self.lb: np.array = None\n",
    "        self.ub: np.array = None\n",
    "        self.init_control: np.array = None  # [nu, nx]\n",
    "    \n",
    "    def f_np(self, x, u): \n",
    "        \"\"\"\"\n",
    "        :param x: np.array, [batch, nx]\n",
    "        :param u: np.array [batch, nu]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def f_torch(self, x, u): \n",
    "        \"\"\"\"\n",
    "        :param x: torch.tensor, [batch, nx]\n",
    "        :param u: torch.tensor [batch, nu]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def f_dreal(self, x, u): \n",
    "        \"\"\"\n",
    "        :param x: np.array[dreal.Variable], [2,]\n",
    "        :param u: np.array[dreal.Variable], [1,]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def in_domain_dreal(self, x, scale=1.):\n",
    "        \"\"\"\n",
    "        :param x: np.array[dreal.Variable], [2,]\n",
    "        \"\"\"\n",
    "        return d.And(\n",
    "            x[0] >= self.lb[0] * scale,\n",
    "            x[0] <= self.ub[0] * scale,\n",
    "            x[1] >= self.lb[1] * scale,\n",
    "            x[1] <= self.ub[1] * scale\n",
    "        )\n",
    "    \n",
    "    def on_boundry_dreal(self, x, scale=2.):\n",
    "        condition1 = d.And(\n",
    "            x[0] >= self.lb[0] * scale * 0.99,\n",
    "            x[0] <= self.ub[0] * scale * 0.99,\n",
    "            x[1] >= self.lb[1] * scale * 0.99,\n",
    "            x[1] <= self.ub[1] * scale * 0.99\n",
    "        )\n",
    "        condition2 = d.Not(\n",
    "            d.And(\n",
    "                x[0] >= self.lb[0] * scale * 0.97,\n",
    "                x[0] <= self.ub[0] * scale * 0.97,\n",
    "                x[1] >= self.lb[1] * scale * 0.97,\n",
    "                x[1] <= self.ub[1] * scale * 0.97\n",
    "            )\n",
    "        )\n",
    "        return d.And( condition1, condition2 )\n",
    "    \n",
    "    def sample_in_domain(self, n, scale=1.): pass\n",
    "    \n",
    "    def sample_out_of_domain(self, n, scale=1.): pass\n",
    "    \n",
    "    \n",
    "class DoubleIntegrator(Benchmark):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name='dint'\n",
    "        self.nx = 2\n",
    "        self.nu = 1\n",
    "        self.lb = np.array([-4., -4.])\n",
    "        self.ub = np.array([4., 4.])\n",
    "        self.init_control = np.zeros([self.nu, self.nx])\n",
    "    \n",
    "    def f_np(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: [batch, 2], np.array\n",
    "            x[:,0] = \\theta\n",
    "            x[:,1] = \\dot\\theta\n",
    "        :param u: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return np.stack([x[:, 1], u[:,0]], axis=1)\n",
    "    \n",
    "    def f_torch(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: [batch, 2], torch.tensor\n",
    "          x[:,0] = \\theta\n",
    "          x[:,1] = \\dot\\theta\n",
    "        :param u: [batch, 1], torch.tensor\n",
    "        \"\"\"\n",
    "        return torch.stack([x[:, 1], u[:,0]], dim=1)\n",
    "    \n",
    "    def f_dreal(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: np.array[dreal.Variable], [2,]\n",
    "        :param u: np.array[dreal.Variable], [1,]\n",
    "        \"\"\"\n",
    "        return np.stack([x[1], u[0]], axis=0)\n",
    "    \n",
    "    def sample_in_domain(self, n, scale=1.):\n",
    "        return np.random.uniform(self.lb * scale, self.ub * scale, (n, self.nx))\n",
    "    \n",
    "    def sample_out_of_domain(self, n, scale=1.):\n",
    "        x = np.random.uniform(-1, 1, (n, self.nx))\n",
    "        xnorm = np.maximum( np.abs(x[:, 0]) / (self.ub[0] * scale),  np.abs(x[:, 1]) / (self.ub[1] * scale) )\n",
    "        x = x / xnorm[:, None]\n",
    "        noise = np.random.uniform(0, 0.5, (n, self.nx))\n",
    "        x = x + np.sign(x) * noise\n",
    "        return x\n",
    "    \n",
    "class VanderPol(Benchmark):\n",
    "    def __init__(self, mu = 1.):\n",
    "        super().__init__()\n",
    "        self.name = 'vanderpol'\n",
    "        self.nx = 2\n",
    "        self.nu = 1\n",
    "        self.lb = np.array([-2., -2.])\n",
    "        self.ub = np.array([2., 2.])\n",
    "        self.init_control = np.array([[0, 1.]])\n",
    "        self.mu = mu\n",
    "        \n",
    "    def f_np(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: [batch, 2], np.array\n",
    "        :param u: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        x1 = x[:,0]\n",
    "        x2 = x[:, 1]\n",
    "\n",
    "        x1d = x2\n",
    "        x2d =  - x1 + self.mu * (1 - x1 ** 2) * x2 + u[:,0]\n",
    "\n",
    "        return np.stack([x1d, x2d], axis=1)\n",
    "    \n",
    "    def f_torch(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: [batch, 2], torch.tensor\n",
    "          x[:,0] = \\theta\n",
    "          x[:,1] = \\dot\\theta\n",
    "        :param u: [batch, 1], torch.tensor\n",
    "        \"\"\"\n",
    "        x1 = x[:,0]\n",
    "        x2 = x[:, 1]\n",
    "\n",
    "        x1d = x2\n",
    "        x2d =  - x1 + self.mu * (1 - x1 ** 2) * x2 + u[:,0]\n",
    "\n",
    "        return torch.stack([x1d, x2d], dim=1)\n",
    "    \n",
    "    def f_dreal(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: np.array[dreal.Variable], [2,]\n",
    "        :param u: np.array[dreal.Variable], [1,]\n",
    "        \"\"\"\n",
    "        x1 = x[0]\n",
    "        x2 = x[1]\n",
    "\n",
    "        x1d = x2\n",
    "        x2d =  - x1 + self.mu * (1 - x1 ** 2) * x2 + u[0]\n",
    "\n",
    "        return np.array([x1d, x2d])\n",
    "    \n",
    "    def sample_in_domain(self, n, scale=1.):\n",
    "        return np.random.uniform(self.lb * scale, self.ub * scale, (n, self.nx))\n",
    "    \n",
    "    def sample_out_of_domain(self, n, scale=1.):\n",
    "        x = np.random.uniform(-1, 1, (n, self.nx))\n",
    "        xnorm = np.maximum( np.abs(x[:, 0]) / (self.ub[0] * scale),  np.abs(x[:, 1]) / (self.ub[1] * scale) )\n",
    "        x = x / xnorm[:, None]\n",
    "        noise = np.random.uniform(0, 0.5, (n, self.nx))\n",
    "        x = x + np.sign(x) * noise\n",
    "        return x     \n",
    "    \n",
    "    \n",
    "\n",
    "class InvertedPendulum(Benchmark):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = 'pendulum'\n",
    "        self.nx = 2\n",
    "        self.nu = 1\n",
    "        self.lb = np.array([-2., -4.])\n",
    "        self.ub = np.array([2., 4.])\n",
    "        self.init_control = np.array([[-23.58639732,  -5.31421063]])\n",
    "        \n",
    "        self.G = 9.81  # gravity\n",
    "        self.L = 0.5  # length of the pole\n",
    "        self.m = 0.15  # ball mass\n",
    "        self.b = 0.1  # friction\n",
    "        \n",
    "    def f_np(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: [batch, 2], np.array\n",
    "            x[:,0] = \\theta\n",
    "            x[:,1] = \\dot\\theta\n",
    "        :param u: [batch,], np.array\n",
    "        \"\"\"\n",
    "        theta = x[:,0]\n",
    "        thetad = x[:,1]\n",
    "        \n",
    "        thetadd = self.G / self.L * np.sin(theta) - self.b / (self.m * self.L**2) * thetad + u[:,0] / (self.m * self.L**2)\n",
    "        return np.stack([thetad, thetadd], axis=1)\n",
    "    \n",
    "    def f_torch(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: [batch, 2], torch.tensor\n",
    "            x[:,0] = \\theta\n",
    "            x[:,1] = \\dot\\theta\n",
    "        :param u: [batch,], torch.tensor\n",
    "        \"\"\"\n",
    "        theta = x[:,0]\n",
    "        thetad = x[:,1]\n",
    "        \n",
    "        thetadd = self.G / self.L * torch.sin(theta) - self.b / (self.m * self.L**2) * thetad + u[:,0] / (self.m * self.L**2)\n",
    "        return torch.stack([thetad, thetadd], dim=1)\n",
    "    \n",
    "    def f_dreal(self, x, u):\n",
    "        \"\"\"\n",
    "        :param x: np.array[dreal.Variable], [2,]\n",
    "        :param u: np.array[dreal.Variable], [1,] \n",
    "        \"\"\"\n",
    "        theta = x[0]\n",
    "        thetad = x[1]\n",
    "        thetadd = self.G / self.L * d.sin(theta) - self.b / (self.m * self.L**2) * thetad + u[0] / (self.m * self.L**2)\n",
    "        return np.array([thetad, thetadd])\n",
    "        \n",
    "    def sample_in_domain(self, n, scale=1.):\n",
    "        return np.random.uniform(self.lb * scale, self.ub * scale, (n, self.nx))\n",
    "    \n",
    "    def sample_out_of_domain(self, n, scale=1.):\n",
    "        x = np.random.uniform(-1, 1, (n, self.nx))\n",
    "        xnorm = np.maximum( np.abs(x[:, 0]) / (self.ub[0] * scale),  np.abs(x[:, 1]) / (self.ub[1] * scale) )\n",
    "        x = x / xnorm[:, None]\n",
    "        noise = np.random.uniform(0, 0.5, (n, self.nx))\n",
    "        x = x + np.sign(x) * noise\n",
    "        return x\n",
    "    \n",
    "class BicycleTracking(Benchmark):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = 'tracking'\n",
    "        self.v = 6.\n",
    "        self.l = 1.\n",
    "        self.nx = 2\n",
    "        self.nu = 1\n",
    "        \n",
    "        self.lb = np.array([-0.7, -0.7])\n",
    "        self.ub = np.array([0.7, 0.7])\n",
    "        self.init_control = np.array([[-0.8471 , -1.6414]])\n",
    "        \n",
    "    def f_np(self, x, u):\n",
    "        y1 = self.v * np.sin(x[:,1])\n",
    "        y2 = self.v * np.tan(u[:,0]) / self.l - np.cos(x[:,1]) / (1 - x[:,0])\n",
    "        return np.stack([y1, y2], axis=-1)\n",
    "    \n",
    "    def f_torch(self, x, u):\n",
    "        y1 = self.v * torch.sin(x[:,1])\n",
    "        y2 = self.v * torch.tan(u[:,0]) / self.l - torch.cos(x[:,1]) / (1 - x[:,0])\n",
    "        return torch.stack([y1, y2], dim=-1)\n",
    "    \n",
    "    def f_dreal(self, x, u):\n",
    "        y1 = self.v * d.sin(x[1])\n",
    "        y2 = self.v * d.tan(u[0]) / self.l - d.cos(x[1]) / (1 - x[0])\n",
    "        return np.array([y1, y2])\n",
    "    \n",
    "    def sample_in_domain(self, n, scale=1.):\n",
    "        return np.random.uniform(self.lb * scale, self.ub * scale, (n, self.nx))\n",
    "    \n",
    "    def sample_out_of_domain(self, n, scale=1.):\n",
    "        x = np.random.uniform(-1, 1, (n, self.nx))\n",
    "        xnorm = np.maximum( np.abs(x[:, 0]) / (self.ub[0] * scale),  np.abs(x[:, 1]) / (self.ub[1] * scale) )\n",
    "        x = x / xnorm[:, None]\n",
    "        noise = np.random.uniform(0, 0.1, (n, self.nx))\n",
    "        x = x + np.sign(x) * noise\n",
    "        return x\n",
    "        \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T15:08:27.231269Z",
     "start_time": "2024-03-08T15:08:26.835297Z"
    }
   },
   "id": "c4f0f1b88825c0f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_dreal_for_benchmark():\n",
    "    sys = DoubleIntegrator()\n",
    "    x_dreal = dreal_var(sys.nx, name='x')\n",
    "    u_dreal = dreal_var(sys.nu, name='u')\n",
    "    xd_dreal = sys.f_dreal(x_dreal, u_dreal)\n",
    "\n",
    "    x = np.random.randn(1, sys.nx)\n",
    "    u = np.random.randn(1, sys.nu)\n",
    "    xd = sys.f_np(x, u)[0]\n",
    "    print('x:', x)\n",
    "    print('u:', u)\n",
    "    print('xd:', xd)\n",
    "\n",
    "    conditions = []\n",
    "    conditions += [x_dreal[i] == x[0][i] for i in range(sys.nx)]\n",
    "    conditions += [u_dreal[i] == u[0][i] for i in range(sys.nu)]\n",
    "    conditions += [xd_dreal[i] <= xd[i]+1e-3 for i in range(sys.nx)]\n",
    "    conditions += [xd_dreal[i] >= xd[i]-1e-3 for i in range(sys.nx)]\n",
    "    fsat = d.And(\n",
    "      *conditions\n",
    "    )\n",
    "\n",
    "    r = d.CheckSatisfiability(fsat, 0.001)\n",
    "    print(r)\n",
    "\n",
    "\n",
    "test_dreal_for_benchmark()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3aefc0a2a58d4296"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# import dreal as d\n",
    "\n",
    "class TanhNetwork(nn.Module):\n",
    "    def __init__(self, dims, final_act='tanh'):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.final_act = final_act\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(dims)-2):\n",
    "            layers.append( nn.Linear(dims[i], dims[i+1]) )\n",
    "            layers.append( nn.Tanh() )\n",
    "        \n",
    "        layers.append( nn.Linear(dims[-2], dims[-1]) )\n",
    "        if final_act == 'tanh':\n",
    "            layers.append( nn.Tanh())\n",
    "        elif final_act == 'sigmoid':\n",
    "            layers.append( nn.Sigmoid() )\n",
    "        else:\n",
    "            raise \"Not Implemented\"\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).squeeze(-1)\n",
    "    \n",
    "    def forward_with_grad(self, x):\n",
    "        \"\"\"\n",
    "        This function should only be called when dims[-1]=1\n",
    "        :param x: [batch, input_dim]\n",
    "        :return:\n",
    "            y: [batch,]\n",
    "            grad: [batch, input_dim], grad[i] = d/dx f(x_i)\n",
    "        \"\"\"\n",
    "        assert self.dims[-1] == 1\n",
    "        y = self(x)\n",
    "\n",
    "        # grad f(x)\n",
    "        jacob = torch.autograd.functional.jacobian(\n",
    "            self,\n",
    "            (x,),\n",
    "            create_graph=True\n",
    "        )\n",
    "        jacob = jacob[0]  # [batch, batch, dim]\n",
    "        grad = torch.diagonal(jacob).T  # diagonal(shape (3,3,2)) gives (2,3), but we want (3,2)\n",
    "        \n",
    "        return y, grad\n",
    "    \n",
    "    def get_param_pair(self):\n",
    "        \"\"\"get_param_pair\n",
    "        \n",
    "        :return:\n",
    "        ws: Weight variables in relu network\n",
    "        bs: Bias variables in relu network\n",
    "        ws and bs must have the same length\n",
    "        \n",
    "        \"\"\"\n",
    "        ws = []\n",
    "        bs = []\n",
    "        \n",
    "        for name, param in self.layers.named_parameters():\n",
    "        \n",
    "            if \"weight\" in name:\n",
    "                # print(param.shape)\n",
    "                ws.append(param.cpu().detach().numpy())\n",
    "            elif \"bias\" in name:\n",
    "                # print(param.shape)\n",
    "                bs.append(param.cpu().detach().numpy())\n",
    "        if len(bs) == 0:\n",
    "            bs = [\n",
    "                np.zeros([w.shape[0]]) for w in ws\n",
    "            ]\n",
    "        return ws, bs\n",
    "    \n",
    "    def forward_dreal(self, x):\n",
    "        \"\"\"\n",
    "        Construct dreal expression for the neural network\n",
    "        :param x: np.array[dreal.Variable]\n",
    "        \"\"\"\n",
    "        ws, bs = self.get_param_pair()\n",
    "        for w, b in zip(ws[:-1], bs[:-1]):\n",
    "            x = dreal_elementwise(w @ x + b, d.tanh)\n",
    "\n",
    "        x = ws[-1] @ x + bs[-1]\n",
    "        if self.final_act == 'tanh':\n",
    "            x = dreal_elementwise(x, d.tanh)\n",
    "        elif self.final_act == 'sigmoid':\n",
    "            x = dreal_elementwise(x, dreal_sigmoid)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T15:08:28.920330Z",
     "start_time": "2024-03-08T15:08:28.913932Z"
    }
   },
   "id": "fa812d44ac0f3268"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_dreal_for_network(act='tanh'):\n",
    "    net = TanhNetwork([2, 1], act)\n",
    "    x_dreal = dreal_var(2)\n",
    "    y_dreal = net.forward_dreal(x_dreal)\n",
    "\n",
    "    x = np.random.randn(1,2)\n",
    "    y = torch_to_np(net(np_to_torch(x)))[0]\n",
    "\n",
    "    print('x:', x, y)\n",
    "\n",
    "    conditions = []\n",
    "    conditions += [x_dreal[i] == x[0][i] for i in range(len(x))]\n",
    "    conditions += [y_dreal[0] >= y - 1e-3, y_dreal[0] <= y + 1e-3] # avoid floating point issue in pytorch\n",
    "    print(conditions)\n",
    "    fsat = d.And(\n",
    "      *conditions\n",
    "    )\n",
    "\n",
    "    r = d.CheckSatisfiability(fsat, 0.001)\n",
    "    print(r)\n",
    "\n",
    "test_dreal_for_network('tanh')\n",
    "print('---')\n",
    "test_dreal_for_network('sigmoid')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2738a2bfc20bc883"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 system:Benchmark,\n",
    "                 w_dim,\n",
    "                 c_dim, \n",
    "                 alpha=0.01,\n",
    "                 batch_size=16,\n",
    "                 path_sampled=8,\n",
    "                 integ_threshold=200,\n",
    "                 norm_threshold=1e-2,\n",
    "                 dt=0.05,\n",
    "                 learning_rate=1e-2):\n",
    "        \"\"\"\n",
    "\t\t:param system:\n",
    "\t\t:param w_dim: [nx, ..., 1]\n",
    "\t\t:param c_dim: [nx, ..., nu]\n",
    "\t\t:param alpha: hyperparameter\n",
    "\t\t:param batch_size: number of sample points for each iteration\n",
    "\t\t:param path_sampled: number of paths to sample for each iteration\n",
    "\t\t:param integ_threshold: see simulate_trajectory\n",
    "\t\t:param norm_threshold: see simulate_trajectory\n",
    "\t\t:param dt: time interval for RK-4\n",
    "\t\t\"\"\"\n",
    "        if not os.path.exists('./%s' % system.name):    os.mkdir('./%s' % system.name)\n",
    "        if not os.path.exists('./%s/plots' % system.name):  os.mkdir('./%s/plots' % system.name)\n",
    "        if not os.path.exists('./%s/ckpts' % system.name):  os.mkdir('./%s/ckpts' % system.name)\n",
    "        \n",
    "        self.system = system\n",
    "        self.controller = TanhNetwork(c_dim, final_act='tanh')\n",
    "        self.W = TanhNetwork(w_dim, final_act='sigmoid')\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.path_sampled = path_sampled\n",
    "        self.integ_threshold = integ_threshold\n",
    "        self.norm_threshold = norm_threshold\n",
    "        self.dt = dt\n",
    "        self.learning_rate = learning_rate\n",
    "        self.zero = np_to_torch(np.zeros([1, system.nx]))\n",
    "        \n",
    "    \n",
    "    def load_ckpt(self, fname):\n",
    "        ckpt = torch.load(fname, map_location=device)\n",
    "        self.W.load_state_dict(ckpt['W'])\n",
    "        self.controller.load_state_dict(ckpt['C'])\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def simulate_trajectory(self, x=None, max_steps=int(1e7)):\n",
    "        \"\"\"\n",
    "        Simulate a trajectory, terminate the forward rollout if\n",
    "\t\t\t1. the norm is less than self.norm_threshold\n",
    "\t\t\t2. the number of steps exceeds self.integ_threshold\n",
    "\t\t\t3. the trajectory stabilizes\n",
    "\t\t\t4. the number of steps exceeds max_steps\n",
    "\t\t\t\t\t\n",
    "        :param x: torch.tensor with shape [1, nx] or None\n",
    "            the trajectory starts at x, if x is None, the starting point is randomly sampled\n",
    "        :param max_steps: \n",
    "\n",
    "        :return\n",
    "            traj: [steps, nx]\n",
    "            integ: norm integration from the starting point, float\n",
    "            convergence: True, (only false if 2nd condition is satisfied, this is for ploting diverging trajectories)\n",
    "        \"\"\"\n",
    "        integ_acc = 0.\n",
    "        steps = 0\n",
    "        \n",
    "        x = np_to_torch(self.system.sample_in_domain(1)) if x is None else x\n",
    "        x_hist = [x.clone()]\n",
    "        \n",
    "        while True:\n",
    "            steps += 1\n",
    "            norm = torch.linalg.norm(x).item()\n",
    "            integ_acc += norm * self.dt\n",
    "            \n",
    "            if norm < self.norm_threshold \\\n",
    "            or ( len(x_hist) > 10 and torch.linalg.norm(x_hist[-1] - x_hist[-10]) < 1e-3 ) \\\n",
    "            or steps >= max_steps:\n",
    "                return torch.cat(x_hist, dim=0), integ_acc, True\n",
    "            \n",
    "            elif integ_acc > self.integ_threshold:\n",
    "                return torch.cat(x_hist, dim=0), integ_acc, False\n",
    "            \n",
    "            u = self.controller(x)\n",
    "            if self.system.nu == 1:\n",
    "                u = u[:, None]\n",
    "            x = rk4_step(self.system.f_torch, x_hist[-1], u, dt=self.dt)\n",
    "            x_hist.append(x.clone())\n",
    "            \n",
    "    def train(self, iterations=2000):\n",
    "        num_iter = 0\n",
    "        optimizer = torch.optim.Adam(\n",
    "\t\t\tlist(self.W.parameters()) + list(self.controller.parameters()),\n",
    "\t\t\tlr=self.learning_rate\n",
    "\t\t)\n",
    "        scheduler = StepLR(optimizer, step_size=500, gamma=0.8)\n",
    "        \n",
    "        # plotting, only for 2d data\n",
    "        xx_plot = np.linspace(self.system.lb[0] * 2, self.system.ub[0] * 2, 3000)\n",
    "        yy_plot = np.linspace(self.system.lb[1] * 2, self.system.ub[1] * 2, 3000)\n",
    "        X_plot, Y_plot = np.meshgrid(xx_plot, yy_plot)\n",
    "        xys_plot = np.stack([X_plot, Y_plot], axis=-1).reshape([-1, 2])\n",
    "        xys_plot = np_to_torch(xys_plot)\n",
    "        \n",
    "        for unused in range(iterations):\n",
    "            num_iter += 1\n",
    "            vs = []\n",
    "            xs = []\n",
    "            \n",
    "            for _ in range(self.path_sampled):\n",
    "                traj, integ, _ = self.simulate_trajectory()\n",
    "                xs.append(traj[0])\n",
    "                vs.append(integ)\n",
    "            xs = torch.stack(xs, dim=0)\n",
    "            vs = np_to_torch(np.array(vs))\n",
    "            \n",
    "            critic_loss = 0.\n",
    "            actor_loss = 0.\n",
    "            \n",
    "            # W(0) = 0\n",
    "            W0 = self.W(self.zero)\n",
    "            # critic_loss += 5. * torch.square(W0) # pendulum\n",
    "            critic_loss += 5. * torch.square(W0) # tracking\n",
    "            \n",
    "            # Approximate W(x) = tanh(alpha * V(x))\n",
    "            Wx = self.W(xs)\n",
    "            What = torch.tanh(self.alpha * vs)\n",
    "            critic_loss += F.mse_loss(Wx, What)\n",
    "            \n",
    "            \n",
    "            # Physics-Informed Loss (PDE residual)\n",
    "            # xs = np_to_torch(self.system.sample_in_domain(self.batch_size, scale=1. if num_iter < 1000 else 2.))\n",
    "            xs = np_to_torch(self.system.sample_in_domain(self.batch_size, scale=1.))\n",
    "            Wx, grad_Wx = self.W.forward_with_grad(xs)\n",
    "            us = self.controller(xs)\n",
    "            if self.system.nu == 1:\n",
    "                us = us[:, None]\n",
    "            fx = self.system.f_torch(xs, us)\n",
    "            xnorm = torch.norm(xs, p=2, dim=1)\n",
    "            residual = torch.sum(grad_Wx * fx.detach(), dim=1) + self.alpha * xnorm * (1 + Wx) * (1 - Wx)\n",
    "            critic_loss += 1. * torch.mean(torch.square(residual))\n",
    "            \n",
    "            \n",
    "            # Controller Loss\n",
    "            grad_Wx = grad_Wx / torch.linalg.norm(grad_Wx, dim=1, keepdim=True)\n",
    "            actor_loss += 1. * torch.mean(\n",
    "                torch.sum(grad_Wx.detach() * fx, dim=1)\n",
    "            )\n",
    "\n",
    "            \n",
    "            # Barrier\n",
    "            # if num_iter < 1000:\n",
    "            xs = np_to_torch(self.system.sample_out_of_domain(self.batch_size, scale=2.))\n",
    "            Wx = self.W(xs)\n",
    "            \n",
    "            # critic_loss += 2. * F.l1_loss(Wx, torch.ones_like(Wx).to(device)) # pendulum\n",
    "            critic_loss += 2. * F.l1_loss(Wx, torch.ones_like(Wx).to(device)) # tracking\n",
    "                \n",
    "            loss = .5 * actor_loss + critic_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if num_iter % 10 == 0:\n",
    "                print(num_iter, critic_loss.item(), actor_loss.item())\n",
    "            \n",
    "            if num_iter % 20 == 0:\n",
    "                vals = pipe(xys_plot, self.W, torch_to_np)\n",
    "                fig = plt.figure()\n",
    "                ax = plt.axes()\n",
    "                im = ax.contourf(X_plot, Y_plot, np.reshape(vals, [len(xx_plot), len(yy_plot)]), levels=100)\n",
    "                fig.colorbar(im)\n",
    "                \n",
    "                for _ in range(5):\n",
    "                    path, _, convergence = self.simulate_trajectory(max_steps=3000)\n",
    "\n",
    "                    if convergence:\n",
    "                        path = torch_to_np(path)\n",
    "                        plt.plot(path[0, 0], path[0, 1], 'r+')\n",
    "                        plt.plot(path[:, 0], path[:, 1], 'o-', markersize=1, linewidth=0.5)\n",
    "                plt.gca().set_aspect('equal')\n",
    "                plt.savefig('./%s/plots/%d.png' % (self.system.name, num_iter))\n",
    "                plt.close()\n",
    "                \n",
    "            if num_iter % 1000 == 0:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        'W': self.W.state_dict(),\n",
    "                        'C': self.controller.state_dict()\n",
    "                    },\n",
    "                    './%s/ckpts/%d.pth' % (self.system.name, num_iter)\n",
    "                )\n",
    "                \n",
    "    def check_lyapunov(self, level=0.9, scale=2., eps=0.5):\n",
    "        W0 = self.W(self.zero).squeeze().item()\n",
    "        x = dreal_var(self.system.nx)\n",
    "        x_norm = d.Expression(0.)\n",
    "        lie_derivative_W = d.Expression(0.)\n",
    "\n",
    "        # construct xnorm and f(x, u)^T \\nabla_x W(x)\n",
    "        u = self.controller.forward_dreal(x)\n",
    "        fx = self.system.f_dreal(x, u)\n",
    "        Wx = self.W.forward_dreal(x)[0]\n",
    "\n",
    "        # construct x_norm and <fx, \\grad_x W(x)>\n",
    "        for i in range(self.system.nx):\n",
    "            x_norm += x[i] * x[i]\n",
    "            lie_derivative_W += fx[i] * Wx.Differentiate(x[i])\n",
    "        x_norm = d.sqrt(x_norm)\n",
    "\n",
    "        condition = d.And(\n",
    "            x_norm >= eps,\n",
    "            self.system.in_domain_dreal(x, scale),\n",
    "            Wx <= level,\n",
    "            d.Or(\n",
    "                lie_derivative_W >= 0,\n",
    "                Wx <= W0\n",
    "            )\n",
    "        )\n",
    "        r1 = d.CheckSatisfiability( condition, 0.0001 )\n",
    "        \n",
    "        r2 = d.CheckSatisfiability(\n",
    "            d.And(\n",
    "                self.system.on_boundry_dreal(x, scale=scale),\n",
    "                Wx <= level\n",
    "            ),\n",
    "            0.0001\n",
    "        )\n",
    "        return r1, r2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T15:08:31.286384Z",
     "start_time": "2024-03-08T15:08:31.281962Z"
    }
   },
   "id": "62960d38b8bebac7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Double Integrator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e7a92f36bca9d47"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# system = DoubleIntegrator()\n",
    "# trainer = Trainer(system,\n",
    "#                   [system.nx, 20, 20, 1],\n",
    "#                   [system.nx, 10, 10, system.nu],\n",
    "#                   alpha=0.05,\n",
    "#                   dt=0.01,\n",
    "#                   norm_threshold=1e-2,\n",
    "#                   integ_threshold=150,\n",
    "#                   batch_size=64,\n",
    "#                   path_sampled=8,\n",
    "#                   learning_rate=2e-3)\n",
    "# trainer.train(3000)\n",
    "# trainer.check_lyapunov(0.7, 2.0, 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T00:24:45.479085Z",
     "start_time": "2024-02-22T00:24:45.476036Z"
    }
   },
   "id": "782256fe28871d32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Vander Pol"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67ceee041eee0e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The training is slow for the first (roughly) 80 iterations since the dynamics forms an orbit and the path sampling always exceeds the maximum steps.\n",
    "\n",
    "\n",
    "# vanderpol\n",
    "# system = VanderPol()\n",
    "# trainer = Trainer(system,\n",
    "#                   [system.nx, 30, 30, 1],\n",
    "#                   [system.nx, 30, 30, system.nu],\n",
    "#                   alpha=0.1,\n",
    "#                   dt=0.01,\n",
    "#                   norm_threshold=1e-2,\n",
    "#                   integ_threshold=50,\n",
    "#                   batch_size=64,\n",
    "#                   path_sampled=8,\n",
    "#                   learning_rate=3e-3)\n",
    "# \n",
    "# trainer.train(3000)\n",
    "# trainer.check_lyapunov(0.5, 2.0, 0.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7129d24a0dd24167"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Inverted Pendulum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a85bc296f849d58"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# system = InvertedPendulum()\n",
    "# trainer = Trainer(system,\n",
    "#                   [system.nx, 20, 20, 1],\n",
    "#                   [system.nx, 5, 5, system.nu],\n",
    "#                   alpha=0.2,\n",
    "#                   dt=0.003,\n",
    "#                   norm_threshold=1e-2,\n",
    "#                   integ_threshold=150,\n",
    "#                   batch_size=64,\n",
    "#                   path_sampled=8,\n",
    "#                   learning_rate=2e-3)\n",
    "# trainer.train(3000)\n",
    "# trainer.check_lyapunov(0.7, 2.0, 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-13T23:03:42.259636Z",
     "start_time": "2024-02-13T23:03:42.253654Z"
    }
   },
   "id": "dd9af7e007c6678d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Bicycle Tracking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38d0366862766385"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# system = BicycleTracking()\n",
    "# trainer = Trainer(system,\n",
    "#                   [system.nx, 20, 20, 1],\n",
    "#                   [system.nx, 10, 10, system.nu],\n",
    "#                   alpha=1.5,\n",
    "#                   dt=0.003,\n",
    "#                   norm_threshold=1e-2,\n",
    "#                   integ_threshold=50,\n",
    "#                   batch_size=64,\n",
    "#                   path_sampled=8,\n",
    "#                   learning_rate=2e-3)\n",
    "# trainer.train(3000)\n",
    "# trainer.check_lyapunov(0.4, 2.0, 0.2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67e08734c0fbb6cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np_to_torch(np.array([[-0.083125000000000004,1.]]))\n",
    "Wx, gradWx = trainer.W.forward_with_grad(x)\n",
    "u = trainer.controller(x)[:,None]\n",
    "fx = trainer.system.f_torch(x, u)\n",
    "print(Wx)\n",
    "print(torch.sum(gradWx * fx, dim=1))\n",
    "print(trainer.W(trainer.zero))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ed0adb081bbc28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
